\chapter{Background}

% - 7-22 pages
% - research on history, "story" on the topic
% - introduce necessary science/engineering to understand approach.
%   - decide on what is commonly known and what is not. -> supervisor should check on that

%%%%% start writing here!

Searching online for \gls{iacacr} quickly leads to terms such as \textquote{snowflake}, \textquote{pet} or \textquote{cattle} \cite{snowflake_servers}.
In this context, the former two are synonyms and refer to directly/manually managed (configured and maintained) machines. Typically, they are unique, must never go down (even for maintenance), are \textquote{hand fed} (changes are applied manually), and it is not feasible to redeploy them \cite{pets_vs_cattle}. The term \textquote{cattle} is used when referring to machines that are never directly interacted with. Instead, all administrative interactions with them are automated.
The approach of treating machines as cattle aims to standardize and therefore reduce the overall administrative effort for large amounts of servers. Standardization is also a requirement for automation. When operating on such a larger scale, it is easier to maintain some kind of automation framework and unify the deployment of machines than to administrate each server manually. At the same time, cattle machines are replaceable by design, which is not the case for pet machines.
But even before those terms were introduced, some data centers were already too large to maintain each server manually.
This chapter will guide through a part of the history of datacenter technologies, explain how they work (whenever they are necessary to understand the further chapters), and identify their primary issues.
% During this thesis, the terms node, host, as well as server, are mostly used as synonyms and can refer to virtual or physical machines. When neither virtual nor physical is specified, machine refers to hardware. 
% "Golden images" for virtual machines, but physical machines as well. Those valuable images contained necessary drivers and configurations

\section{Bare-metal}
In the early times of data centers, they required quite the administrative effort. Reinstalling an operating system on a server required one administrator to be physically located close to the server, some kind of installation media, a monitor, and at least a keyboard. Since both monitor and keyboard were rarely used, \gls{kvmswitchesacr} (not to be confused with the Linux kernel virtual machine with the same abbreviation) quickly gained a foothold. \Gls{kvmswitchesacr} had one set of IO devices like monitor and keyboard attached on one side and several servers on the other side. Pressing the corresponding button, the complete set of IO-devices would be \textquote{automatically} detached from whatever server it was previously connected to and attached to the machine the button corresponds to.
\newline
Those devices still exist and evolved into network-attached versions, which means they don't require administrators to press buttons on the device, and instead of a dedicated set of IO devices per handful of servers, they allow administrators to use the ones attached to their workstation. So these devices introduce some kind of remote control for servers, including visual feedback.
Their main issue is not the dedicated cabling they require for each server, but the limited amount of servers they can be attached to. The largest KVM-Switches have 64 ports \cite{64_port_kvm_switches}, meaning they can be attached to 64 machines simultaneously. For data centers with more machines, this type of management does not scale very well (even financially, since those 64-port switches tend to cost as much as a new car \cite{64_port_kvm_switches}).
\newline
Instead of installing each operating system manually, two methods for unattended installations emerged: One is the creation of so-called \textquote{golden images}, where all needed software is preinstalled, settings are baked in, correct drivers are in place and so on \cite{what_golden_image}. The image is then just copied over to new machines. The other is closely related but has a different name for each operating system. Examples are \textquote{preseed} for Debian, \textquote{setupconfig} for Windows, and \textquote{cloud-init} for various operating systems including Ubuntu \cite{cloud_init_releases}. Under the hood they all work the same: Instead of asking the user each question during the setup with the default installation medium, the answers are predefined in a special file. This file can then be baked into the installation medium or separately (even on-demand via network).
\newline
With those methods, administrators only need to attach the installation medium, configure the machine to boot from it, and power on the machine. While this does save a large amount of time already, it still requires manual interactions with the machine.
\newline
To further automate machine installations, technologies like \gls{tftpacr} (1981), \gls{pxeacr} (1984), \gls{bootpacr} (1985) emerged and concluded in the development of \gls{dhcpacr} (1993). Only when Intel released \gls{wolacr} in 1997 and PXE 2.0 as part of its Wired-for-Management system in 1998 it was possible to fully network-boot a device.
%TODO source: wikipedia \url{https://en.wikipedia.org/wiki/Preboot\_Execution\_Environment} and links within, f.e. https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol#History
\newline
\Gls{pxeacr} uses \gls{dhcpacr} to assign an IP address to a \gls{nicacr}. When the \gls{nicacr} receives a so-called \textquote{magic packet} containing its MAC address during the \gls{wolacr} process, it triggers the machine to power-on. Depending on the \gls{biosacr} or \gls{uefiacr} (which is a newer implementation of the former) settings, the machine starts with its configured boot-order. For network-boot this means an embedded \gls{nbpacr} (for example the widespread network boot loaders PXELINUX or iPXE), which are the networking equivalent to what \gls{grubacr} and the Windows Boot Loader are for local disks: It downloads a kernel from a (network) resource, loads it into memory and finally (chain-)boots the actual \gls{osacr} \cite{preboot_execution_environment} \cite{understanding_bare_metal_service}.
\newline
The combination of all those technologies finally allows to remotely power on a machine, boot a kernel via network instead of a local disk - all while using the \gls{nicacr} as the interface for those abilities, outsourcing the bootstrapping and scaling to the network infrastructure.
\newline
But there are still some issues with those technologies:
\newline
When a machine had an error that made it unresponsible for remote access (like SSH) but did not power down the machine either, again an administrator was required to physically attend the server and manually resolve the issue.
\newline
The next generation of servers (since 1998) had such a remote control integrated into their mainboard, also rendering \gls{kvmswitchesacr} obsolete because this new method scales vertically: Every new server has an embedded chip that acts as an integrated remote control. Unifying those efforts into a single standard for the whole industry, Intel published a specification called \gls{ipmiacr} around that. Instead of \textquote{only} the ability of remote-controlling a server with keyboard, mouse, and monitor, \GLS{ipmiacr} allows administrators to mount ISO images remotely (in a way similar to network-boot, but a different approach), change the boot order, read hardware sensor values during both power-on- and -off-times and even control the power state of the machine. Especially the last part now allows administrators to maintain machines completely remotely via network, making physical attendance only required for changing physical parts of the infrastructure. 
The aforementioned embedded chips are called \gls{bmcacr} and the surrounding technology is often called \gls{oobacr} or \gls{lomacr}. Even though these are universal terms for the chips and the technology, most hardware manufacturers have different names for their specific toolset. Examples are (not needed further in this thesis) DRAC from DELL EMC, ILO from HPE, and IMM from IBM. Not only the product names are different: Many features have different names but are also doing the same. Probably due to their original purpose, those chips are not embedded in every modern mainboard, but only available in server- and enterprise-desktop-mainboards. Since \gls{ipmiacr} adds an attack vector to the device, having such a chip in consumer devices decreases its security.
\newline
There are two different sets of problems solved with all those technologies:
The combination of \gls{ipmiacr} and \gls{lomacr} allows administrators to debug a machine even on the other side of the planet (or even in space \cite{spacex_servers}) by giving them remote input/output capabilities as well as power cycle control and the ability to get hardware sensor information. Since every vendor has a slightly different approach to their \gls{lomacr} interface, it is almost impossible to automate it universally.
The network-boot technologies around \gls{pxeacr} on the other side are capable of automating a high number of servers in parallel but do not provide necessary Input/Output for debugging issues.
\newline
Yet, together these solutions enable administrators to automate hardware provisioning at scale while at the same time providing them with remote low-level debugging tools.
\newline
These standards are the state-of-the-art tools for remote-server-administration for several years now, along with \gls{sshacr}. They mostly solve the administration scaling problem and form the base for other tools (more on them later).
\newline
% TODO
% power down a machine is necessary -> separation of concerns -> inefficient usage
% vertical scaling of software -> distributed software
Sometimes, it is necessary to power a machine down. Be it for exchanging/adding hardware components or other maintenance. Therefore a best practice separates different workloads on different machines. This has the advantage that for example powering down a web server does not impact a database server. At the same time, it has the downside that servers are not used efficiently: When the database has almost no load, but the web-server is close to its limit, the load cannot be distributed properly between them. This is where virtualization comes in.

% TODO add the following sources into text above
%  - \url{https://en.wikipedia.org/wiki/Out-of-band_management}
%    - out-of-band management (OOB) == lights-out-management (LOM)
%  - PXE (DHCP with DNS, Gateway) (wired-for-management by ms and intel)
%    - TFTP used to download kernel
%    - NBP (Network Bootstrap Program) is the networking equivalent to GRUB and LiLo (linuxloader) -> it loads the kernel into memory before booting it (\url{https://docs.openstack.org/ironic/latest/user/architecture.html})
%  - retrieving vendor from mac is possible (\url{https://macvendors.com/api}) - can this be used for ipmi/... distinction?

\section{Virtualization}
Even though IBM shipped its first production computer system capable of full virtualization in 1966 \cite{ibm_first_hypervisor}, it still took several decades until the \textquote{official} break-though of virtualization technologies. Only then were machines powerful enough for virtualization that makes sense in terms of performance, leading to lower management overhead, fewer unused system resources, and therefore overall cost savings \cite{xen_killer}.
Starting 2005, Intel and AMD added hardware virtualization support to their processors and the Xen hypervisor was published. Other hypervisors followed: Microsoft Hyper-V and Proxmox Virtual Environment were both published in 2008. The initial release of VMware's ESX hypervisor even dates back to 2001 but evolved to its successor ESXi in 2004. The first version of the Linux kernel which contained the \gls{kvmacr} hypervisor (not to be mistaken with the equal abbreviation for keyboard, video, mouse described earlier - from this point onwards, \gls{kvmacr} always refers to the hypervisor) was published in 2007.
\newline
Apart from the previously stated advantages, the virtualization of machines enables migrations of them to another host without downtime (\textquote{live-migration}), finally allowing them to evacuate hosts before maintenance work. The same feature also drastically improves disaster recovery capabilities \cite{definition_server_virtualization}.
\newline
But the use of hypervisors and clustering them for live-migration and other cross-node functionalities has downsides as well: Vendor lock-in since the different \gls{vmacr} formats are not compatible (there are some migration/translation tools, but best practices for production environments advise against them), license/support fees in addition to the hardware support fees and requiring additional expertise for the management software.
\newline
Yet, 100 percent of the fortune 500 and 92 percent of all business used (server-)virtualization technologies in 2019 \cite{adoption_of_virtualization_technologies} \cite{vmware_accelerate_it} \cite{spiceworks_state_of_virtualization}. And VMware claims, that 80 percent of all virtualized workloads run on VMware technology \cite{vmware_accelerate_it}, whereas Statista estimates their share to only 20 percent \cite{statista_vmware}.

% TODO add sources to above text:
%  - \url{https://searchservervirtualization.techtarget.com/definition/server-virtualization}, \url{https://docs.oracle.com/cd/E26996\_01/E18549/html/BHCJAIHJ.html}
%    benefits of virtualization
%    - server consolidation
%    - simplified physical infrastructure
%    - reduced hardware and facilities costs
%    - greater server versatility
%    - improved management
%    - easier disaster recovery
%    - by being software defined: improved remoting capabilities
%    downsides:
%    - vendor-lock in
%    - licence fees
%    - another part requiring experience
%    - multiple kernels running
%    - minor performance decrease -> statistics

% TODO: additional concepts could be explained here, allowing later references
% - converged infrastructure: combine storage and "normal" network traffic into the same network % -> no fiber channel switches anymore
%   - simpler and easier to manage, easier and cheaper to purchase
% - hyper-converged infrastructure: merge storage nodes and compute nodes; or in other words: every node has storage and a powerful processor.
%   - hope for less over- and underuse of resources, easier to scale
%   - no additional appliances for data protection, data de-duplication (integrated into HCI software)
%   - performance guarantee, predictable at all times
% - disaggregated hyper-converged infrastructure/hybrid hyper-converged infrastructure
% - composable infrastructure

\section{Cloud}
The term cloud describes a group of servers that are accessed over the internet and the services (for example databases) that run on those servers \cite{cloudflare_what_is_cloud}. These servers are located in one or (most often) multiple data centers. There are three types of clouds: Private clouds, which refers to servers and services which are only available internally (i.e. only shared within the organization). The second type consists of public clouds, which refers to publicly available services (i.e. shared with other organizations) \cite{cloudflare_what_is_private_cloud}. And lastly, there are hybrid clouds, which mix both of the previous types. All of these have five main attributes in common: They allow for on-demand allocation, self-service interfaces, migration between hosts, as well as replication and scaling of services, % lecture notes, VSYS, during bachelor
and \cite{microsoft_what_is_private_cloud}].
\newline
The public cloud era began with the launch of Amazon's Web Services in 2006. Since then, it evolved into one of the biggest markets with a yearly capacity of \$270 billion and an estimated growth of almost 20 percent \cite{gartner_forecast_cloud_spending}. The current capacity exceeds even the market capitalization of Norway \cite{indexmundi_ranking}. Considering the amount of revenue generated (at least \$40 billion \cite{indexmundi_ranking}, it is obvious why the likes of Microsoft (in 2010) and Google (in 2013) followed Amazon into the cloud market \cite{cbinsights_multi_cloud_strategies}.
\newline
Cloud computing can generate these high rates of revenue because they take advantage of economy of scale, very efficient sharing of resources, as well as a focus of a huge amount of developer effort into a relatively low amount of features (in contrast to every organization implementing the same feature set over and over for themselves) \cite{forbes_cloud_economy_scale}.
\newline
Apart from financial and developer efficiency, clouds have a long list of advantages and disadvantages \cite{dsl_for_iac}, but these are out of scope here.
\newline
The high degree of automation and possibilities for scaling within a cloud environment made it possible to automate the process of this scaling. The time required to provision (and remove) new nodes plays an important role during autoscaling. This is where containers come in.

%TODO Where to put the following part? It's an introduction between clouds and IaC...:
%Since all cloud providers have their own self-service portals, \gls{apiacr}s and in most cases different features, migration between providers for whatever reason are often very difficult and time consuming.

%TODO Add image with in-house/on-premise infra, off-premise infra, private cloud, public cloud, and hybrid cloud

\section{Containers}
While the idea of containers exists for quite some time already (2006 as so-called cgroups, 2007 with LXC, \cite{cgroups} \cite{lxc}, it only reached mainstream popularity with the release of docker in 2013 \cite{docker_initial_release}. The main difference between a \gls{vmacr} and a container is the kernel: The former has a dedicated kernel, which runs in parallel with the hypervisor kernel (yet controlled by it). The latter however shares the kernel of the underlying operating system, thus not requiring a kernel to be loaded for each new instance. As a result, the provisioning speed is dramatically reduced: While \gls{vmacr}s are not uncommon to exceed 60 seconds until being fully available, containers only require the time the operating system needs to start a new process, which is sub-second in most cases \cite{vps_provisioning_times}.
\newline
Containers also (almost completely) solve the \textquote{works-on-my-machine} syndrome, where the developer machine is different from (for example) the production system to the extent that a new feature might only work on either, but not both.
\newline
Some go even as far as saying containers are the future of cloud computing \cite{containers_future_cloud_computing} \cite{devopsonline_serverless_future} \cite{alibabacloud_serverless_future} \cite{ttpsc_serverless_future} (or maybe the future of container computing looks different then previously thought \cite{azure_modular_datacenter} \cite{google_patent_container}).
\newline
Docker Inc. also introduced a cross-machine management tool called Swarm, which allows users to describe the desired state, which the engine tries to turn into reality (consistently). It was accompanied by Google's Kubernetes in 2014 on the short list of container orchestrators. Kubernetes is based on another (internal) software by Google called Borg, which is the underlying system for services like YouTube, Gmail, Google Docs, and their web search. The company had no place to put the open-source software, so they partnered with the Linux Foundation to create the \gls{cncfacr} \cite{cncf_guide_landscape}. The \gls{cncfacr} Landscape has since evolved into a multi-trillion-dollar ecosystem, so the Kubernetes story only scrapes its surface. The cloud-native world has even been labeled as Cloud 2.0 \cite{alibabacloud_serverless_future}.
\newline
Orchestrators like Swarm and Kubernetes, along with the cloud providers become more complex with the more features they get, and since the high amount of automation leads to an ever-changing state, several ways to describe the desired state were developed: The birth of \gls{iacacr}.

%TODO myths:
%   - virt has significantly lower perf than bare-metal
%   - bare-metal container is better than container in VM (f.e. lower overhead) -> no live-migration on bare-metal, chunks of bare-metal (sizing)

\section{Infrastructure-as-Code}
\Gls{iacacr} is the result of multiple factors:
\begin{itemize}
  \item Software development encompasses more than running it, for example, a build pipeline, testing, and compliance. All of this has to be documented.
  \item Documentation is hard to hold up to date \cite{software_engineers_documentation} \cite{software_documentation_issues}. This is not special to orchestrators or cloud providers but is true for all software.
  \item The only source of information that cannot lie (as in being out of date) is the source code.
  \item Scaling (infrastructure) leads to standardized objects.
  \item To have multiple instances of the same type of nodes, they have to be provisioned the same.
  \item The only (reliable) way to do something the same way over and over is to script/program them.
  \item Infrastructure becomes more and more software-defined, reducing required physical changes required for changes in the infrastructure (which enables automation).
  \item Version-control-systems like git are well established and allow for rollbacks, collaboration, reviews and, actionability \cite{iac_oreilly}. This improves the quality and enables further automation.
\end{itemize}
The practice of \gls{iacacr} is best described as finding a compromise between human- and machine-readable languages to describe and directly manage the infrastructure.
\newline
Due to the trend towards software-defined everything \cite{sde_deloitte} \cite{sde_researchgate}, the advantages gained by using \gls{iacacr} grow steadily. As soon as a certain software has an \gls{apiacr}, it can be integrated into \gls{iacacr}. Since the created code only describes how and when to interact with which \gls{apiacr} and not the actual implementation behind it, some kind of orchestrator is required which processes the requests and runs the actual workflows behind the endpoints.
\newline
There are two ways to implement those workflows. The first is a push-based mechanism, where the orchestrator triggers actions on other parts of the system (for example commanding a hypervisor to create a \gls{vmacr}). The other is a pull-based mechanism, where those subsystems (like a hypervisor) periodically ask the orchestrator whether tasks have to be completed \cite{infoworld_puppet_chef_ansible_salt}.
\newline
These mechanisms not only apply to the interaction between the orchestrator and the subsystems but between the source code and the orchestrator as well.
\newline
To increase the capabilities of the orchestrator or in other words enable more things to get defined via software, middle or abstraction layers are introduced. An example of this is the hypervisor that acts as an intermediate layer between hard- and software-defined machines. The deployment (and configuration) of that middleware is not within the scope of most \gls{iacacr} frameworks and is outsourced. This layer must be as easy to deploy as possible, making it hard to bring in mistakes and staying as flexible as possible for further configuration via software.
\newline
It is obvious that not everything can be software-defined, since some physical objects (like cables) have to be physically placed \cite{iac_bare_metal}. Robots could be used, but in most cases, this is something human workers do. Whether the configuration is correct can often be detected/measured from software. On the other hand, technologies like \gls{fpgaacr}s can even change the CPU architecture via software - so the future might have some surprises in store.
\newline
One of the hardest things about applying \gls{iacacr} to bare metal is the complex management and interactions between the multiple \gls{apiacr}s. On one side are the \textquote{external} protocols and interfaces like \gls{dhcpacr}, \gls{tftpacr}, \gls{httpacr}, \gls{dnsacr} and \gls{sshacr}. On the other side are the \gls{osacr}s and the features they provide for automation \cite{iac_bare_metal}.
These range from being able to install the \gls{osacr} in an unattended way, over scriptable settings (or better: Non-scriptable ones - looking at you Windows) to compatibility with widespread instance initialization methods like cloud-init \cite{cloudinit_docs}.
\newline
Another major difference in bare metal are the firmwares. Since they dictate the available features and how the interface of the hardware looks like \gls{apiacr}, it is important to have them in the correct version \cite{iac_bare_metal}.

%TODO Currently, asset-management and IaC are too different domains, where either hard- or software-defined parts of the infrastructure are managed.

\section{Domain-Specific Language}
As described in the previous chapter, \gls{iacacr} requires an equally machine- and human-readable language. These modeling languages can best be described as \gls{dslacr}s as their only purpose is to describe very specific things \cite{dsl_web_app}. Even among those the domains they can (and want to) describe vary a lot. Additionally, they differ in several properties, for example, whether they are graphical or textual; But since \gls{iacacr} is by definition \textquote{as code}, and code is text-based, corresponding \gls{dslacr}s have to be text-based as well. Examples for well-known \gls{dslacr}s in other domains are SQL and CSS \cite{dsl_slides}.
\newline
In contrast to a \gls{gplacr} (not to be confused with the license), its domain-specific counterpart promises higher success rates even with less experience and significantly higher closeness of mapping \cite{comparing_gpl_dsl}. Especially the last attribute helps developers to simplify their state descriptions. Another major advantage of using a \gls{gplacr} is the ecosystem of tools; Because they are well supported by IDEs, they have powerful features like syntax highlighting, code refactoring, and testing support \cite{iac_oreilly}.
\newline
Another differentiating characteristic is the approach, which can be imperative or declarative; Imperative languages describe actions to be done, for example, \textquote{create X additional instances of Y}, whereas declarative languages are used to describe the desired state, for example, \textquote{I want X instances of Y}. When using the latter, it is the job of the orchestrator to compare the current state against the described desired state and conclude the required actions themselves \cite{dsl_for_iac}.
Because \gls{iacacr} always aims at describing the whole state, declarative languages are better fitted for this task \cite{iac_oreilly}. They also have the property of being idempotent: If applied multiple times, the result does not change \cite{iac_oreilly}. In order to describe the (whole) state of infrastructure, the declarative way is also more intuitive. It is the same way humans would describe a state (for example \textquote{I see three apples} instead of three times \textquote{I see an(other) apple}).
\newline
Some \gls{dslacr}s (called \textquote{internal}) in this field are based on another language like XML, \gls{jsonacr}, or \gls{yamlacr} \cite{iac_oreilly}. This includes both sub- and supersets of them. Libraries are internal \gls{dslacr}s as well \cite{dsl_oreilly}. \textquote{External} \gls{dslacr}s on the other hand are not directly related to other languages \cite{dsl_oreilly}. An example is the \gls{hclacr} used by Terraform \cite{iac_oreilly} \cite{dsl_oreilly}.
\newline
An additional difference between the tools and languages is how they are applied. Some use a push-based mechanism, where f.e. the orchestrator initiates communication with nodes and applies changes. Others use a pull-based mechanism, where the nodes need to watch their configuration at the orchestrator level and execute the required actions locally so they become configured as intended. The design decision of push or pull applies to other things as well: How code changes are loaded into the orchestrator for example.
\newline
In contrast to a \gls{gplacr}, a \gls{dslacr} allows better separation of infrastructure code from other code \cite{dsl_slides}. Additionally, they are more context-driven, which makes them easier to work with for domain experts and users \cite{dsl_oreilly}. Their syntax is smaller and well-defined too, which makes them less complex as well \cite{dsl_oreilly}.
\newline
By using a modeling language, much of the complexity behind the \gls{dslacr} is abstracted away. It is instead shifted to the technology experts that create those languages and the surrounding tools \cite{dsl_oreilly}.
\newline
In an ideal world, a \gls{dslacr} for \gls{iacacr} is not a limiting factor; For example, it is not limited to full usage of only virtualization, containers nor bare-metal. It should support all of those cases and also allow hybrid scenarios. Additionally, it should be able to describe both small and large environments, while the required effort should increase less than linear. Furthermore, an ideal \gls{dslacr} should not lock into a single vendor, but empower migrations and cross-provider scenarios wherever the user sees fit. This includes the license and owner of the language; It should not be left in the hands of a single organization, but a group (of several independent organizations/individuals). While a single owning organization tends to reflect itself in the software \cite{conways_law}, a group of organizations or a committee can help in finding a much more universal solution. On the other hand, the more stakeholders are involved, the harder a compromise is to find.
