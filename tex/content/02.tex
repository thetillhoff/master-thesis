\chapter{Background}

% - 7-22 pages
% - research on history, "story" on the topic
% - introduce necessary science/engineering to understand approach.
%   - decide on what is commonly known and what not. -> %TODO supervisor should check on that

% ---

%%%%% start writing here!

Searching online for \gls{iacacr} quickly leads to the terms such as \textquote{snowflake}, \textquote{pet} or \textquote{cattle}. \url{https://dzone.com/articles/martin-fowler-snowflake}
In this context, the former two are synonyms and refer to directly/manually managed (configured and maintained) machines. Typically, they are unique, can never be down and "hand fed" - it is not feasable to redeploy them. \url{http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/} The latter is used when referring to machines, that are never directly interacted with; All administrative interactions with them are automated.
The approach of treating machines as cattle aims to unify and therefore reduce the administration effort for large amounts of servers. When operating on such larger scales, it is easier to maintain some kind of automation framework and unify the deployment of machines than to administrate each server manually. At the same time, cattle-machines are replacable by design, which is not the case for pet-machines.
But even before those terms were introduced, some datacenters were already too large to maintain each server manually.
This chapter will guide through a part of history of datacenter technologies, explain how they work whenever they are necessary to understand the further chapters and identify their primary issues.
%TODO "Golden images" for virtual machines, but physical machines as well. Those valuable images contained necessary drivers and configurations

%TODO: "node" == "host"? == "machine" == server

\section{Bare-metal}
In the early times of datacenters, they required quite the administrative effort. Reinstalling an operating system on a server required one administrator to be physically located close to the server, some kind of installation media, a monitor and at least a keyboard. Since both monitor and keyboard were rarely used, \gls{kvmswitchesacr} quickly gained foothold. \Gls{kvmswitchesacr} had one set of IO-devices like monitor and keyboard attached on one side and several servers on the other side. Pressing a corresponding button, the complete set of IO-devices would be \textquote{automatically} detached from whatever server it was previously connected to and attached to the machine the button refers to.
\newline
Those devices still exist and evolved into network-attached versions, which means they don't require administrators to press buttons on the device and instead of dedicated set of IO-devices per handful of servers, they allow administrators to use the ones attached to their workstation. So these devices introduce some kind of remote control for servers, including visual feedback.
Their main issue is not the dedicated cabling they require to each server, but the limited amount of servers they can be attached to. The largest KVM-Switches have 64 ports \url{https://kvm-switch.de/en/category-335/from-16-Port-KVM-Switches/64-Port-KVM-Switches/}, meaning they can be attached to 64 machines. For datacenters with more machines, this type of management doesn't scale very well (even financially, since those 64-port switches tend to cost as much as a new car).
\newline
Instead of installing each operating system manually, two methods for unattended installations emerged: One is the creation of so-called \textquote{golden images}, where all needed software is preinstalled, settings are baked in, correct drivers are in place and so on \url{https://opensource.com/article/19/7/what-golden-image}. The other is closely related and has a different name for each operating system. Examples are \textquote{preseed} for debian, \textquote{setupconfig} for windows, \textquote{cloud-init} for various operating systems including ubuntu (2008, \url{https://github.com/canonical/cloud-init/releases?after=ubuntu-0.3.1}). Under the hood they all work the same: Instead of asking the user each question during setup, the answers are predefined in a special file. This file can be baked in into the golden image or seperately (even on-demand via network).
\newline
With those methods, administrators only need to attach the installation medium, configure the machine to boot from it and power-on the machine. While this does save a large amount of time already, it still requires manual interactions with the machine.
\newline
To further automate machine installations, technologies like \gls{tftpacr} (1981), \gls{pxeacr} (1984), \gls{bootpacr} (1985) emerged and concluded in the development of \gls{dhcpacr} (1993). Only when Intel released the \gls{wolacr} in 1997 and PXE 2.0 as part of its Wired-for-Management system in 1998 it was possible to fully network-boot a device.
%TODO source: wikipedia \url{https://en.wikipedia.org/wiki/Preboot\_Execution\_Environment} and links within, f.e. https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol#History
\newline
\Gls{pxeacr} uses \gls{dhcpacr} to assign an ip-address to a \gls{nicacr}. When the \gls{nicacr} receives a so-called \textquote{magic packet} during the \gls{wolacr} process, it triggers the machine to power-on. Depending on the BIOS/UEFI settings, the machine starts with its configured boot-order, for network-boot this means an embedded \gls{nbpacr} (f.e. pxelinux or ipxe), which is like a networking equivalent to what GRUB is for local disks: It downloads a kernel from a network resource, loads it into memory and finally (chain-)boots it [\url{https://www.networxsecurity.org/de/mitgliederbereich/glossary/n/network-bootstrap-program.html} \url{https://docs.openstack.org/ironic/latest/user/architecture.html}].
\newline
The combination of all those technologies finally allows to remotely power-on a machine, boot a kernel via network instead of a local disk and makes the \gls{nicacr} the interface for those abilities, outsourcing the bootstrapping and scaling to the network infrastructure.
\newline
But there are still some issues with those technologies: \\
When a machine had an error which made it unresponsible for remote access (like SSH), but didn't power the machine down neither, again an administrator was required to phyiscally attend the server and manually resolve the issue.
\newline
The next generation of servers (since 1998) had such a remote control integrated into their mainboard, rendering \gls{kvmswitchesacr} obsolete, because this new method scales vertical: Every new server, has embedded chip that acts as an integrated remote control. Unifying those efforts into a single standard for the whole industry, Intel published a specification called \gls{ipmiacr} around that. Instead of \textquote{only} the ability of remote-controlling a server with keyboard, mouse and monitor, \GLS{ipmiacr} allows administrators to mount ISO images remotely (in a way like network-boot, but a different approach), change the boot order, read hardware sensor values during both power-on- and -off-times and even control the power-state of the machine. Especially the last part now allowed administrators to maintain serves completely remotely via network, making physical attendance only required for changing physical parts of the intrastructure. The aforementioned embedded chips are called \gls{bmcacr} and the surrounding technology is called \gls{oobacr} or \gls{lomacr}. Even though these are universal terms for the chips and the technology, most hardware manufacturers have their own name for their specific toolset, like DRAC for DELL, ILO for HPE and IMM for IBM. Probably due to their origin an purpose, those chips are not embedded in every modern mainboard, but only available in server- and enterprise-desktop-mainboards.
\newline
There are two different sets of problems solved with all those technologies:
The combination of \gls{ipmiacr} and \gls{lomacr} allows administrators to debug a machine even on the other side of the planet. %todo: even space? is there a source for that?
Network-booting on the other side helps with automating a high number of servers in parallel, but doesn't really help with debugging errors.
\newline
These standards are to state-of-the-art remote-server-administration-tools for several years, along with \gls{sshacr}. They mostly solve the administration scaling problem or form the base for other tools. %TODO source
\newline
% power off a machine is necessary -> separation of concerns -> inefficient usage
% vertical scaling of software -> distributed software
% TODO
Sometimes, it is necessary to power a machine down. Be it for exchanging/adding hardware components or other maintenance. Therefore a best-practice seperates different workloads on different machines. This has the advantage that f.e. powering down a web-server, doesn't impact a database-server. At the same time it has the downside that servers are not efficiently used: When the database has almost no load, the web-server, it still blocks





%  - IPMI
%    - on
%    - off
%    - list-sensors
%    - get-sensor (detailed)
%    - attachISO
%    - detachISO
%    - bootOrder
%    - (create, delete), virt-equivalent
%  - BMC
%  - \url{https://en.wikipedia.org/wiki/Out-of-band_management}
%    - out-of-band management (OOB) == lights-out-management (LOM)
%  - PXE (DHCP with DNS, Gateway) (wired-for-management by ms and intel)
%    - TFTP used to download kernel
%    - NBP (Network Bootstrap Program) is the networking equivalent to GRUB and LiLo (linuxloader) -> it loads the kernel into memory before booting it (\url{https://docs.openstack.org/ironic/latest/user/architecture.html})
%  - datacenter architecture
%    - Top-of-rack-switch
%  - retrieving vendor from mac is possible (\url{https://macvendors.com/api}) - can this be used for ipmi/... distinction?

\section{Virtualization}
Even though IBM shipped its first production computer system capable of full virtualization in 1966 [\url{https://en.wikipedia.org/wiki/Hypervisor}], it still took several decades until the "official" break-though of virtualization technologies. Only then were machines powerful enough for virtualization that makes sense in terms of performance, leading to lower management overhead, fewer unused system resources and therefore overall cost savings. [Loftus, Jack (December 19, 2005). "Xen virtualization quickly becoming open source 'killer app'". TechTarget. Retrieved October 26, 2015. -> \url{http://searchdatacenter.techtarget.com/news/1153127/Xen-virtualization-quickly-becoming-open-source-killer-app}]
Starting 2005, Intel and AMD added hardware virtualization to their processors and the Xen hypervisor was published. Microsofts Hyper-V followed in 2008, as well as the Proxmox Virtual Environment. The initial release of VMwares ESX hypervisor dates back to 2001, but evolved to ESXi in 2004. The first version of the linux kernel containing the \gls{kvmacr} hypervisor (not to be mistaken with the equal abbreviation for keyboard, video, mouse described earlier - from this point onwards, \gls{kvmacr} always refers to the hypervisor) was published in 2007.
\newline
Apart from the previously stated advantages, virtualization allowed for live-migrations of machines to another host without downtime, finally allowing to evacuate a machine prior to maintenance work. The same feature also drastically improves disaster recovery capabilities [\url{https://searchservervirtualization.techtarget.com/definition/server-virtualization}].
\newline
But the use of hypervisors and clustering them for live-migration and other cross-node functionalities has a downside as well: Vendor lock-in, since the different \gls{vmacr} formats are not compatible (there are some migration/translation tools, but best practices for production environments advise against them), licence / support fees in addition to the hardware support fees and requiring additional expertise for the management software.
\newline
Yet, 100 percent of the fortune 500 and 92 percent of all business used (server-)virtualization technologies in 2019 [\url{https://www.statista.com/statistics/1139931/adoption-virtualization-technologies-north-america-europe/}, \url{https://www.vmware.com/files/pdf/VMware-Corporate-Brochure-BR-EN.pdf}, \url{https://www.spiceworks.com/marketing/reports/state-of-virtualization/}].

% On a sidenote, VMWare claims that 80 percent of all virtualized workloads run on VMWare technology [\url{https://www.vmware.com/files/pdf/VMware-Corporate-Brochure-BR-EN.pdf}]. %TODO whereas Statista estimates their share to only 20 percent [\url{https://www.statista.com/statistics/915091/global-server-share-physical-virtual/}].


%  - hypervisor and vm
%  - \url{https://www.statista.com/statistics/915091/global-server-share-physical-virtual/}
%    - 2018, 57,2 % of servers worldwide were pyhsical
%      2019, 55,6 % of servers worldwide were physical
%    - 2018, vmware accounted for 20.1 % of servers
%    - 2019, vmware accounted for 20.8 % of servers
%  - \url{https://www.statista.com/statistics/1139931/adoption-virtualization-technologies-north-america-europe/}
%    - 2019 92% of all business use virtualization technologies
%
%  - \url{https://searchservervirtualization.techtarget.com/definition/server-virtualization}, \url{https://docs.oracle.com/cd/E26996\_01/E18549/html/BHCJAIHJ.html}
%    benefits of virtualization
%    - server consolidation
%    - simplified physical infrastructure
%    - reduced hardware and facilities costs
%    - greater server versatility
%    - improved management
%    - easier disaster recovery
%    - by being software defined: improved remoting capabilities
%    downsides:
%    - vendor-lock in
%    - licence fees
%    - another part requiring experience
%    - multiple kernels running
%    - minor performance decrease -> statistics

% TODO: additional concepts could be explained here, allowing later references
% - converged infrastructure: combine storage and "normal" network traffic into the same network % -> no fiber channel switches any more
%   - simpler and easier to manage, easier and cheaper to purchase
% - hyper-converged infrastructure: merge storage nodes and compute nodes; or in other words: every node has storage and a powerful processor.
%   - hope for less over- and underuse of resources, easier to scale
%   - no additional appliances for data protection, data de-duplication (integrated in HCI software)
%   - performance guarantee, predictable at all times
% - disaggregated hyper-converged infrastructure / hybrid hyper-converged infrastructure
% - composable infrastructure
% - ...




\section{Cloud}
The term cloud describes a group of servers, that are accessed over the internet and the software and databases that runs on those servers [\url{https://www.cloudflare.com/learning/cloud/what-is-the-cloud/}]. These servers are located in one or multiple datacenters. There are three types of clouds: Private clouds, which refers to servers and services which are only available internally (i.e. only shared within the organization). The second type are public clouds, which refers to publicly available services (i.e. shared with other organizations) [\url{https://www.cloudflare.com/learning/cloud/what-is-a-private-cloud/}]. And lastly, there are hybrid clouds, which mix both of the previous types. All of these have five main attributes in common: They allow for on-demand allocation, self-service interfaces, migration between hosts, as well as replication and scaling of services [lecture notes, VSYS, during bachelor, and \url{https://azure.microsoft.com/en-us/overview/what-is-a-private-cloud/}].
\newline
The public cloud era began with the launch of Amazon's Web Services in 2006. Since then, it evolved into one of the biggest markets with a yearly capacity of \$270 billion and an estimated growth of almost 20 percent [Gartner \url{https://www.gartner.com/en/newsroom/press-releases/2021-04-21-gartner-forecasts-worldwide-public-cloud-end-user-spending-to-grow-23-percent-in-2021}]. The current value even exceeds the market capitalization of Norway [\url{https://www.indexmundi.com/facts/indicators/CM.MKT.LCAP.CD/rankings}]. Considering the amount of revenue generated (at least \$40 billion [\url{https://www.indexmundi.com/facts/indicators/CM.MKT.LCAP.CD/rankings}]), it is obvious why the likes as Microsoft (in 2010) and Google (in 2013) followed Amazon into the cloud market [\url{https://www.cbinsights.com/research/amazon-google-microsoft-multi-cloud-strategies/#history}].
\newline
Cloud computing is able to generate these high rates of revenue because they take advantage of economy of scale, very efficient sharing of resources, as well as a combination of a huge amount of developer effort into a low amount of features (in contrast to every organization implementing the same featureset over and over for themselves) [\url{https://en.wikipedia.org/wiki/Cloud_computing}].
\newline
Apart from financial and developer efficiency, clouds have a long list of advantages and disadvantages [Domain-specific language for infrastructure as code]. %TODO Leave like this?
\newline
The high degree of automation and possibilities for scaling within a cloud made it possible to scale automatically. The time required to provision (and deprovision) new nodes plays an important role for autoscaling. This is where containers come in.
%TODO Where to put the following part? Its an introduction between clouds and IaC...
%\newline
%Since all cloud-providers have their own self-service portals, \gls{apiacr}s and in most cases different features, migration between providers for whatever reason are often very difficult and time consuming.

%TODO Add image with in-house/on-premis infra, off-premise infra, private cloud, public cloud and hybrid cloud

%  Public clouds have quite the arsenal of pros and cons [Domain-specific language for infrastructure as code].
%  - vps
%  - \url{https://www.vpsbenchmarks.com/labs/provisioning_times}
%    vps provisioning times
%    - hetzner: 14s
%    - GCP: 20s
%    - Amazon EC2: 23s
%    - DigitalOcean: 24s
%    - Scaleway: 32s
%    - Alibaba: 41s
%    - Amazon Lightsail: 48s
%    - Oracle Cloud: 60s
%    - Vultr: 62s
%    - OVHcloud: 72s
%    - IBM Cloud: 86s
%    - MS Azure: 102s

\section{Containers}
While the idea of containers exists for quite some time already (2006 as cgroups, 2007 with LXC, \url{https://en.wikipedia.org/wiki/Cgroups}, \url{https://en.wikipedia.org/wiki/LXC}), it only reached mainstream popularity with the release of docker in 2013 [\url{https://en.wikipedia.org/wiki/Docker_(software)}]. The main difference between a \gls{vmacr} and a container is the kernel: The former has its own dedicated kernel, which runs in parallel with the hypervisors kernel (yet controlled by it). The latter however shares the kernel of the underlying operating system, thus not requiring a kernel to be loaded for each new instance. As a result, the provisioning speed is dramatically reduced: While \gls{vmacr}s are not uncommon to exceed 60 seconds until being fully available, containers only require the time the operating system needs to start a new process, which is sub-second in most cases [\url{https://www.vpsbenchmarks.com/labs/provisioning_times}].
\newline
Containers also (almost completely) solve the \textquote{works on my machine} syndrome, where the developer machine is different to (f.e.) the production system to the extend that a new feature might only work on either, but not both.
\newline
Some go even as far as saying containers are the future of cloud computing [\url{https://www.cloudpassage.com/articles/containers-future-cloud-computing/}, \url{https://www.devopsonline.co.uk/is-serverless-the-future/}, \url{https://www.alibabacloud.com/blog/why-is-serverless-the-future-of-cloud-computing_597191}, \url{https://ttpsc.com/en/blog/why-serverless-is-the-future-of-software-and-apps/}] (or maybe the future of container computing looks different then previously thought \url{https://azure.microsoft.com/en-us/blog/introducing-the-microsoft-azure-modular-datacenter/}, \url{https://patents.google.com/patent/US7278273B1/en} ).
\newline
Docker Inc. also introduced a cross-machine management tool called swarm, which allows users to describe a desired state, which the engine tries to realize (at all times). It was accompanied by Google's Kubernetes in 2014 on the short list of container orchestrators. Kubernetes is based on another (internal) software by Google called Borg, which is the underlying system for software like YouTube, Gmail, Google Docs and their web search. The company had no place to put the open source software, so they partnered with the Linux Foundation to create the \gls{cncfacr} [\url{https://www.cncf.io/blog/2018/11/05/beginners-guide-cncf-landscape/}]. The \gls{cncfacr} Landscape has since evolved into a multi-trillion dollar ecosystem, so the Kubernetes story only scrapes its surface. The cloud native world has even been labeled as Cloud 2.0 [\url{https://www.alibabacloud.com/blog/why-is-serverless-the-future-of-cloud-computing_597191}].
\newline
These orchestrators like Swarm and Kubernetes, along with the cloud providers become more complex with the more features they get, and since the high amount of automation leads to an ever-changing state, several ways to describe the desired state were developed.


%TODO myths:
%   - virt has significantly lower perf than bare-metal
%   - bare-metal container is better than container in vm (f.e. lower overhead) -> no live-migration on bare-metal, chunks of bare-metal (sizing)

\section{Infrastructure-as-Code}
\Gls{iacacr} takes advantage of multiple factors:
\begin{itemize}
  \item Software development encompasses more than running it, f.e. a build pipeline, testing and compliance. All of this has to be documented.
  \item Documentation is hard to hold up to date [\hl{How Software Engineers Use Documentation: The State of the Practice}, \hl{Software Documentation Management Issues and Practices: a Survey}]. This is not special to orchestrators or cloud providers, but is true for all software.
  \item The only source of information that cannot lie (i.e. be out of date) is the sourcecode.
  \item Scaling (infrastructure) leads to standardized objects.
  \item In order to have multiple instances of the same type of nodes, they have to be provisioned exactly the same.
  \item The only (reliable) way to something the same way over and over is to script/program them.
  \item Infrastructure becomes more and more software defined, reducing required physical changes required for changes in the infrastructure (which enables automation).
  \item Version-control-systems like git are well established and allow for rollbacks, collaboration, reviews and actionability [\hl{Kief Morris Infrastructure as Code}]. This improves the quality and enables further automation.
\end{itemize}
The practice of \gls{iacacr} is best described as finding a compromise between human- and machine-readable languages to describe and directly manage the infrastructure.
\newline
Due to the trend towards software-defined everything [\hl{Software-defined everything, deloitte}, \hl{Software-defined everything, researchgate, 2017}], the advantages gained by using \gls{iacacr} grow steadily. As soon as a software has an \gls{apiacr}, it can be integrated into \gls{iacacr}. Since the created code only describes how and when to interact with which \gls{apiacr} and not the actual implementation behind it, some kind of orchestrator is required which processes the requests and runs the actual workflows behind the endpoints.
\newline
There are two ways to implement those workflows. The first is a push-based mechanism, where the orchestrator triggers actions on other parts of the system (f.e. commands a hypervisor to create a \gls{vmacr}). The other is a pull-based mechanism, where those subsystems (i.e. a hypervisor) periodically asks the orchestrator whether tasks have to be completed. [\url{https://www.infoworld.com/article/2609482/data-center-review-puppet-vs-chef-vs-ansible-vs-salt.html}]
\newline
These mechanism not only apply to the interaction between the orchestrator and the subsystems, but between the source and the orchestrator as well.
\newline
In order to increase the capabilities of the orchestrator or in other words enable more things to get defined via software, middle- or abstraction-layers are introduced. An example for this is the hypervisor that acts as \gls{apiacr}-gateway between hard- and software-defined machines. The deployment (and configuration) of that middleware (i.e. the hypervisor) is not within the scope of most \gls{iacacr} frameworks and is outsourced. This layer must be as easy to deploy as possible, making it hard to bring in mistakes and staying as flexible as possible for further configuration via software.
\newline
It is obvious, that not everything can be software-defined, since some physical objects (like cables) have to be physically placed [\hl{Can Infrastructure as Code apply to Bare Metal}]. Robots could possibly be used, but in most cases, this is something human workers do. Whether the configuration is correct can often be detected/measured from software. On the other hand, technologies like \gls{fpgaacr}s can even change the CPU architecture via software - so the future might have some surprises in store.
\newline
One of the hardest things about applying \gls{iacacr} to bare metal is the complex management and interactions between the multiple \gls{apiacr}s. On one side are the \textquote{external} protocols and interfaces like \gls{dhcpacr}, \gls{tftpacr}, \gls{httpacr}, \gls{dnsacr} and \gls{sshacr}. On the other side are the \gls{osacr}s and the features they provide for automation [\hl{Can Infrastructure as Code apply to Bare Metal}].
These range from being able to install the \gls{osacr} in an unattended way, over scriptable settings (or better: The non-scriptable ones - looking at you Windows) to compatibility with widespread instance initialization methods like cloud-init [\url{https://cloudinit.readthedocs.io/en/latest/}].
\newline
Another major difference on bare-metal are firmwares. Since they dictate the available version of the hardware \gls{apiacr}s, it is important to have them in the correct version [\hl{Can Infrastructure as Code apply to Bare Metal}].

%TODO Currently, asset-management and IaC are to different domains, where either hard- or software-defined parts of infrastructure are managed.


\section{Domain-Specific Language} % force full name ...
As described in the previous chapter, \gls{iacacr} requires an equally machine- and human-readable language. These modeling languages can best be described as \gls{dslacr}s as their only purpose is to describe very specific things [\hl{A Domain Specific Language to Generate Web Applications}]. Even among those \gls{dslacr}s the domains they can (and want) describe varies a lot. Additionaly, they differ in several properties, for example whether they are graphical or textual; But since \gls{iacacr} is by definition \textquote{as code}, and code is text-based, corresponding \gls{dslacr}s have to be text-based as well. Examples for well-known \gls{dslacr}s in other domains are SQL and CSS \url{http://www.cse.chalmers.se/~bergert/slides/guest_lecture_DSLs.pdf}.
Another property is the approach, which can be imperative or declarative; Imperative languages describe actions to be done, for example \textquote{create X additional instances of Y}, whereas declarative languages are used to describe the desired state \textquote{I want X instances of Y}. When using the latter, it is the orchestrators job to compare the current state against the described desired state and conclude the required actions themselves [\hl{Domain-specific language for infrastructure as code}].
Because \gls{iacacr} always aims at describing the whole state, declarative languages are better fitted for this task \hl{Infrastructure as Code, Kief Morris}. They also have the property of being idempotent: If applied multiple times, the result won't change \hl{Infrastructure as Code, Kief Morris}. In order to describe the state of infrastructure, the declarative way is also more intuitive. It is the same way humans would describe a state (i.e. \textquote{I see three apples} instead of three times \textquote{I see an(other) apple}).
\newline
Some \gls{dslacr}s (called \textquote{internal}) in this field are based on another language as XML, JSON, or YAML \hl{Infrastructure as Code, Kief Morris, second edition}. This includes both sub- and supersets of them. Libraries are internal \gls{dslacr}s as well \hl{hs script, mode}. \textquote{External} \gls{dslacr}s on the other hand are not directly related to other languages \hl{lecture notes MODE}. An example is the \gls{hclacr} used by Terraform \hl{Infrastructure as Code, Kief Morris, second edition} \hl{lecture notes MODE}.
\newline
An additional difference between the tools and languages is how they are applied. Some use a push-based mechanism, where f.e. the orchestrator initiates communication with nodes and applies changes. Others use a pull-based mechanism, where the nodes need to watch the(ir) configuration at the orchestrator level and execute the required actions locally so they become configured as intended. The design decision of push or pull applies to other things as well: How code changes are loaded into the orchestrator for example.
\newline
In contrast to a \gls{gplacr}, a \gls{dslacr} allows better seperation of infrastructure code from other code \url{http://www.cse.chalmers.se/~bergert/slides/guest_lecture_DSLs.pdf}. Additionally, they are more context driven, which makes them easier to work with for domain experts and users \hl{lecture notes MODE}. Their syntax is smaller and well-defined too, which makes them less complex as well.
%TODO source (script of MODE?)
\newline
% domain expert:
% - uses modeling language to create models
% technology experts:
% - creates modeling languages and transformations
In an ideal world, a \gls{dslacr} for \gls{iacacr} is not a limitation factor; For example it is not limited to neither full usage of virtualization, containers nor bare-metal. It should support all of those cases and also allow hybrid scenarios. Additionally, it should be able to describe both small and large environments, while the required effort should increase less than linear. Furthermore, an ideal \gls{dslacr} should not lock into a single vendor, but empower migrations and cross-provider scenarios wherever the user sees fit. This includes the licence and owner of the language; It should not be left in the hands of a single organization, but a group (of several organizations/individuals). While a single owning organization tends to reflect itself in the software [\url{http://www.melconway.com/Home/Conways_Law.html}], a group of organizations or a committee can help in finding a much more universal solution. On the other hand, the more stakeholders are involved, the harder a compromise is to find.
].

% TODO put this somewhere else, f.e. outlook
% \section{Scope}
% config-level
% - vms or not should be able to be answered on config-level and therefore by user not by architecture of iac
% - poc: on-demand/self-service k8s-clusters for users
%   - cli-wrapper around lib, no webgui (as simple as possible)
%   - api for everything <-> everything as a service, all levels (bare-metal, virt, ...)
%   - implies full automation
%   - optional: transparent costs
% mgmt/meta:
% - cost-limit per user/group/departments/...
